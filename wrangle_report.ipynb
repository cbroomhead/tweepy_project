{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intro**\n",
    "\n",
    "This project was creating as a final project of the data wrangling section of the Udacity's Nanodegree for Data Analysis. Built in a Jupyter Notebook, the following tools were used:\n",
    "- pandas\n",
    "- NumPy\n",
    "- requests\n",
    "- tweepy\n",
    "- json\n",
    "\n",
    "The goal of of this project was to demonstrate the ability to gather, assess, and clean data. The project did not require any specific hypothesis testing, predictions, or demonstration of specific data vizualizations. Instead, I wwas tasked to come up with some insights and a visualization to showcase the clean data. \n",
    "\n",
    "Specifically, the project required:\n",
    "- Detect and document at least eight (8) quality issues and two (2) tidiness issues. \n",
    "- Produce at least three (3) insights and one (1) visualization.\n",
    "\n",
    "\n",
    "**Gathering, Assess, Cleaning**\n",
    "\n",
    "The data I used came from three different sources:\n",
    "1) the @rate_dogs twitter archive prior to August 01 2017 was given in .csv format by Udacity's instructor. This contains 2356 rows of data featuring tweet_ids, timestamps, retweeted statuses, media urls, etc. <br />\n",
    "2) what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. <br />\n",
    "3) the responses from the Twitter API Tweepy libary using the tweet_ids given in the twitter archive<br />\n",
    "\n",
    "\n",
    "Each dataset was examined idenpendently and cleaned appropirately based on each observation. Note that there might be more data quality issues that I did not address, but the requirements of the projects only specified above. \n",
    "\n",
    "Here is the summary of the quality issues:\n",
    "\n",
    "| #  |      Quality Issue     |\n",
    "|---|:-------------:|\n",
    "| 1 | convert 'created_at' from string format to datetime |  \n",
    "| 2 | rename df_predictions['id'] to 'tweet_id' to match other datasets |  \n",
    "| 3 | remove 'extended_entities' where Nan, which are rows with no images |  \n",
    "| 4 | remove non original tweets where'retweeted_status' are retweets by @rate_dogs |  \n",
    "| 5 | remove entries where 'favorite_count' is 0, spot checking showed post with more than 0 favorites|  \n",
    "| 6 | remove entries where any of four field'in_reply_to*' are non-Nan or empty |  \n",
    "| 7 | remove entries where df_repdictions['jpg_url'] values are duplicated |  \n",
    "| 8 | remove entris where twitter_archive['expanded_urls'] values are duplicated |  \n",
    "\n",
    "\n",
    "| #  |      Tidiness Issue     |\n",
    "|---|:-------------:|\n",
    "| 1 |  merge all three data frames together on 'tweet_id'. note that the dataframe each had a number of different rows so only only entries with tweet_id presetn in all three will be kept|  \n",
    "| 2 | remove unwanted columns from the newly create df_master dataframe   |  \n",
    "\n",
    "\n",
    "**Analysis**\n",
    "After cleaning the data and integrating it in a master dataframe, I started thinking about what I wanted to analyse. I am particularly interested in the results of the prediciton model from the neural network, so I decide to evaluate that. \n",
    "\n",
    "The summary of those finding can be found in the document called act_report.html which can is folder in the same root directory as this file. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
